{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa73460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'username'\n",
    "access_token = 'token'\n",
    "\n",
    "base_url = 'https://api.github.com'\n",
    "\n",
    "# Number of repositories per day\n",
    "repositories_per_day = 500\n",
    "\n",
    "# Number of days to scrape  \n",
    "days_to_scrape = 60\n",
    "\n",
    "# Calculate the start and end dates for scraping\n",
    "end_date = datetime.now().date()\n",
    "start_date = end_date - timedelta(days=days_to_scrape)\n",
    "\n",
    "# Initialize the repositories list\n",
    "repositories = []\n",
    "\n",
    "# Retry mechanism for API requests\n",
    "retry_strategy = Retry(\n",
    "    total=3,\n",
    "    backoff_factor=0.5,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\"],\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "http = requests.Session()\n",
    "http.mount(\"https://\", adapter)\n",
    "\n",
    "# Variables for tracking total rows and time taken\n",
    "total_rows = 0\n",
    "total_time = timedelta()\n",
    "\n",
    "# Iterate over each day\n",
    "for day in range(days_to_scrape):\n",
    "    current_date = start_date + timedelta(days=day)\n",
    "    formatted_date = current_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Fetch repositories using pagination\n",
    "    page = 1\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    while len(repositories) < repositories_per_day * (day + 1):\n",
    "        # Create the API URL to fetch repositories created on the current day and specific page\n",
    "        url = f'{base_url}/search/repositories?q=created:{formatted_date}&sort=stars&order=desc&per_page=100&page={page}'\n",
    "\n",
    "        # Make the API request with retry logic\n",
    "        headers = {'Authorization': f'token {access_token}'} if access_token else {}\n",
    "        response = http.get(url, headers=headers)\n",
    "        data = response.json()\n",
    "\n",
    "        if 'items' in data:\n",
    "            # Extract repository information from the response\n",
    "            items = data['items']\n",
    "            sorted_items = sorted(items, key=lambda item: item['stargazers_count'], reverse=True)\n",
    "\n",
    "            for item in sorted_items[:repositories_per_day]:\n",
    "                repository = {\n",
    "                    'id': item['id'],\n",
    "                    'full_name': item['full_name'],\n",
    "                    'url': item['html_url'],\n",
    "                    'language': item.get('language', ''),\n",
    "                    'license': item['license']['name'] if item['license'] else '',\n",
    "                    'topics': item.get('topics', []),\n",
    "                    'owner_type': item['owner']['type'],\n",
    "                    'description': item['description'],\n",
    "                    'stars_count': item['stargazers_count'],\n",
    "                    'forks': item['forks'],\n",
    "                    'issues_count': item['open_issues_count'],\n",
    "                    'year': current_date.year, \n",
    "                    'created_at': item['created_at'],\n",
    "                    'updated_at':item['updated_at'],\n",
    "                    \n",
    "                    \n",
    "                }\n",
    "\n",
    "                repositories.append(repository)\n",
    "\n",
    "        page += 1\n",
    "\n",
    "        if 'next' not in response.links:\n",
    "            break\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    time_taken = end_time - start_time\n",
    "    total_time += time_taken\n",
    "\n",
    "    rows_extracted = len(repositories) - total_rows\n",
    "    total_rows = len(repositories)\n",
    "    print(f\"Day {day + 1}: Extracted {rows_extracted} rows in {time_taken}\")\n",
    "\n",
    "# Convert the repositories list to a DataFrame\n",
    "df = pd.DataFrame(repositories)\n",
    "\n",
    "# Select the desired columns\n",
    "df = df[['id', 'full_name', 'url', 'language', 'license', 'topics',\n",
    "         'owner_type', 'description', 'stars_count', 'forks', 'issues_count',\n",
    "         'year', 'created_at', 'updated_at']]\n",
    "\n",
    "row_count = len(df)\n",
    "print(f\"Number of non-duplicated rows: {row_count}\")\n",
    "\n",
    "csv_file_path = 'repositories.csv'\n",
    "excel_file_path = 'repositories.xlsx'\n",
    "\n",
    "df.to_csv(csv_file_path, index=False, encoding='utf-8-sig')\n",
    "df.to_excel(excel_file_path, index=False)\n",
    "\n",
    "print(f\"Data exported to {csv_file_path} and {excel_file_path} successfully.\")\n",
    "print(f\"Total rows extracted: {total_rows}\")\n",
    "print(f\"Total time taken: {total_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa574040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the repositories list to a DataFrame\n",
    "df = pd.DataFrame(repositories)\n",
    "\n",
    "# Select the desired columns\n",
    "df = df[['id', 'full_name', 'url', 'language', 'license', 'topics',\n",
    "         'owner_type', 'description', 'stars_count', 'forks', 'issues_count',\n",
    "         'year', 'created_at', 'updated_at']]\n",
    "\n",
    "row_count = len(df)\n",
    "print(f\"Number of non-duplicated rows: {row_count}\")\n",
    "\n",
    "csv_file_path = 'repositories.csv'\n",
    "excel_file_path = 'repositories.xlsx'\n",
    "\n",
    "df.to_csv(csv_file_path, index=False, encoding='utf-8-sig')\n",
    "df.to_excel(excel_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe82f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('repositories.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c3fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dca51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
